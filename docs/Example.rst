Examples
=====================================

Making a Model
------------------------------

    .. code-block:: python

        class NonlinearModel(PyTorch_Model):

            required_components = ['a','b', 'c', 'd', 'e']

            def init_default_components(self):

                for letter in ['a', 'b', 'c', 'd', 'e']:
                    self.components[letter] = torch.nn.Parameter(torch.Tensor(np.random.normal(0,1,1)))

                self.in_column = 'x'
                self.out_column = 'y_pred'

            def forward(self, message):

                x = message[self.in_column]
                message[self.out_column] = (self.a + self.b*x + self.c*x**2 + self.d*x**3 + self.e*x**4)

                return message

Nonlinear Regression
------------------------------

    .. code-block:: python

        def generate_data(n=1000):

            a = randint(-10,10)
            b = randint(-10,10)
            c = randint(-10,10)
            errors = np.random.normal(0, .5, n)
            x = np.random.rand(n) * 100 - 50
            y = a + b*x + c*x**2

            return Message({'x': x, 'y': y, 'errors': errors}), {'a': a, 'b': b, 'c': c}

    .. code-block:: python

        def get_data(n=1000):

            data, params = generate_data(n)
            train, test = train_test_split(data, test=.25)

            shuffler = ShufflerPipe(train)
            minibatcher = BatchingPipe(shuffler, batch_size=25)
            train_set = TensorPipe(minibatcher, columns=['x','y'])

            test_set = TensorPipe(test, columns=['x','y'])

            return train_set, test_set, params

    .. code-block:: python

        description = "In this experiment, we are training a polynomial model using least squares regression to fit data generated by a random polynomial."
        experiment = Experiment("nonlinear_regression", description=description)

        train_set, test_set, params = get_data(n=1000)

        model = NonlinearModel()

        # Construct training closure and train using ignite
        base_loss = torch.nn.MSELoss()
        loss = lambda batch: base_loss(batch['y_pred'], batch['y'])
        trainer = IgniteJunction(components={'model': model, 'dataset': train_set}, loss=loss, optimizer='Adam', lr=.1)

    .. code-block:: python

        class ModelSaverMetric(Metric):

            def __init__(self, output_transform=lambda x:x, log_interval=100):
                self.model_state = Message()
                Metric.__init__(self, output_transform=output_transform)
                self.log_interval = log_interval

            def iteration_completed(self, engine):
                    iter = (engine.state.iteration-1)
                    if iter % self.log_interval == 0:
                        current_state = Message.from_objects(deepcopy(engine.state.output['state']))
                        current_state['iteration'] = [iter]
                        self.model_state = self.model_state.append(current_state)

            def compute(self):
                # Return most recent model state
                l = len(self.model_state)
                return self.model_state[l-1]

    .. code-block:: python

        model_state_metric = ModelSaverMetric()
        model_state_metric.attach(trainer, 'state')

        x = Message({'x':np.arange(-10,10,.2)}).to_tensors()

        # Run initial evaluation
        y_initial = model(x)['y_pred'].detach().numpy()
        initial_loss = loss(model(test_set[0:250]))
        print("Initial loss on test set: {0}".format(initial_loss))

        # Save initial state of model
        file_path = experiment.open('initial_model', string_only=True)
        initial_state = model.get_state()
        Message.from_objects(initial_state).to('json', path=file_path)

        trainer.train(max_epochs=30)

        final_loss = loss(model(test_set[0:250]))
        print("Final loss on test set:: {0}".format(final_loss))


    .. code-block:: python

        # Visualize functions
        true_model = NonlinearModel(components={'a':[params['a']], 'b': [params['b']], 'c': [params['c']], 'd': [0], 'e': [0]})

        y_true = true_model(x)['y_pred'].detach().numpy()
        y_final = model(x)['y_pred'].detach().numpy()

        # Save model states during training
        file_path = experiment.open('model_states', string_only=True)
        model_states = model_state_metric.model_state
        Message.from_objects(initial_state).to('json', path=file_path)

        fig, ax = plt.subplots()

    .. code-block:: python

        def animate(frame):

            current_state = {'internal': frame['internal'][0], 'external': {}}
            model.set_state(current_state)

            y_predicted = model(x)['y_pred'].detach().numpy()
            xdata = list(x['x'].detach().numpy())
            ydata = list(y_predicted)
            ax.clear()
            ax.plot(xdata, list(y_true), 'r')
            ax.plot(xdata, ydata, 'g')
            title = "Iteration: {0}".format(frame['iteration'][0])

            ax.set_title(title)

        ani = FuncAnimation(fig, animate, model_state_metric.model_state, interval=1000)
        ani.save(experiment.open("models.mp4", string_only=True)) # This will only work if you have ffmpeg installed.
        plt.show()

    .. youtube:: WJw-iIegq3o


Model Selection
------------------------------

    .. code-block:: python

        def make_model(parameters):
            temp_parameters = deepcopy(parameters)
            include = [letter for letter in ['a','b','c','d','e'] if letter in parameters]
            exclude = [letter for letter in ['a','b','c','d','e'] if letter not in parameters]
            for letter in exclude:
                temp_parameters[letter] =  [0]
            model = NonlinearModel(temp_parameters)
            for letter in exclude: # Prevent training from taking place for these parameters
                model.freeze(letter)
            return model

    .. code-block:: python

        def get_trainer(train_set, loss, optimizer, **kwargs):

            def train_from_params(parameters):

                model = make_model(parameters)
                trainer = IgniteJunction(components={'model': model, 'dataset': train_set}, loss=loss, optimizer=optimizer, **kwargs)
                print("Now training model for parameters {0}".format(parameters))
                trainer.train(max_epochs=10)
                evaluator = IgniteJunction(components={'model': model, 'dataset': train_set}, loss=loss, optimizer=optimizer, update_function=default_evaluation_closure, **kwargs)
                print("Now evaluating trained model.")
                return trainer

            return train_from_params

    .. code-block:: python

        class Parameterizer:

            def __init__(self):
                possible_params = ['a','b','c','d','e']
                def generator():
                    for i in reversed(range(5)):
                        for combination in combinations(possible_params,i):
                            params = {param: [0] for param in combination}
                            yield params
                self.generator = generator()

            def __call__(self,past_params, metrics):
                try:
                    params = self.generator.__next__()
                    if params == {}:
                        raise EndHyperparameterOptimization
                    return params

                except StopIteration:
                    raise EndHyperparameterOptimization

    .. code-block:: python

        class AccuracyMetric(Metric):

            def __init__(self, output_transform = lambda x:x):
                Metric.__init__(self, output_transform=output_transform)
                self.reset()

            def reset(self):
                self.l2 = 0.
                self.num_examples = 0

            def update(self, output):
                self.l2 += output['loss']
                self.num_examples += len(output['output'])

            def compute(self):

                if self.num_examples == 0:
                    raise NotComputableError(
                        "Metric must have at least one example before it can be computed."
                    )
                return Message({'average-loss': [self.l2 / self.num_examples]}).to_dataframe()

    .. code-block:: python

        description = "In this experiment, we will compare the performance of different polynomial models when regressed against data generated from a random polynomial."
        experiment = Experiment("model_selection", description=description)

        factory = LocalMemoryFactory(components={
            'trainer': get_trainer(train_set, loss, optimizer='Adam', lr=.1),
            'eval_set': test_set,
            'parameterizer': Parameterizer(),
            'metrics': {'accuracy': AccuracyMetric(), 'model_state': ModelSaverMetric()}
            })

        factory.run()

    .. code-block:: python

        params, metrics = factory.read()
        accuracy_file = experiment.open('accuracy.csv', string_only=True)
        metrics['accuracy'].to('csv', path=accuracy_file)
        model_state_file = experiment.open('model_states.csv', string_only=True)
        metrics['model_state'].to('csv', path=model_state_file)
        params_file = experiment.open('params.csv', string_only=True)
        params.to('csv', path=params_file)

    .. youtube:: yEZ7EvC9Zxc&t=5s

Using Databases
------------------------------

    .. code-block:: python

        columns = [
            Column('x', Float),
            Column('y', Float),
            Column('errors', Float),
        ]

        table = create_table("nonlinear_regression", columns)

    .. code-block:: python

        def write_data(filename='example.sqlite', n=1000):
            try:
                os.remove(filename)
            except FileNotFoundError:
                pass

            engine = create_engine('sqlite:///{0}'.format(filename))
            db = TablePipe(table, engine)

            data, params = generate_data(n)
            db.insert(data)

            with open(filename+"_params", 'w') as f:
                f.write(json.dumps(params))

            db.commit()

    .. code-block:: python

        def load_data(filename='example.sqlite'):
            if not os.path.exists(filename):
                raise FileNotFoundError("File {0} does not exist.".format(filename))
            with open(filename+'_params') as f:
                params = json.load(f)

            engine = create_engine('sqlite:///{0}'.format(filename))
            db = DBPipe(table, engine) # Default query is SELECT * FROM table

            return db, params


    .. code-block:: python

        def get_data(filename='example.sqlite', n=1000):
            if not os.path.exists(filename) and os.path.exists(filename+'_params'):
                write_data(filename, n)

            data, params = load_data(filename)
            looper = LoopingPipe(data)
            cache = CachingPipe(looper, cache_size=1000)
            train, test = train_test_split(cache, test=.25)

            shuffler = ShufflerPipe(train)
            minibatcher = BatchingPipe(shuffler, batch_size=25)
            train_set = TensorPipe(minibatcher, columns=['x','y'])

            test_set = TensorPipe(test, columns=['x','y'])

            return train_set, test_set, params

Model Selection With Databases
------------------------------

    .. code-block:: python

        description = "Model selection for nonlinear regression. We are comparing the regression accuracy of different polynomial models fit to data generated by a random polynomial."
        experiment = Experiment("model_selection_db", db_path=".", description=description)
        # SQL factory
        params_table = create_table('params', columns=[
            Column('a', Integer), Column('b', Integer), Column('c', Integer), Column('d', Integer), Column('e', Integer)
            ])
        metrics_tables = {'accuracy': create_table('accuracy', columns=[Column('average-loss', Float)])}
        # engine = create_engine('sqlite:///model_selection.sqlite')
        engine = experiment.get_engine('factory.sqlite')
        factory = SQLFactory(components={
            'trainer': get_trainer(train_set, loss, optimizer='Adam', lr=.1),
            'eval_set': test_set,
            'parameterizer': Parameterizer(),
            'metrics': {'accuracy': AccuracyMetric()},
            'engine': engine,
            'params_table': params_table,
            'metrics_tables': metrics_tables,
            })

        factory.run()

        params_table = DBPipe('params', factory.engine)
        print(params_table.all())
        accuracy_table = DBPipe('accuracy', factory.engine)
        print(accuracy_table.all())
